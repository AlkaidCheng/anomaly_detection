{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cf3a9a-f034-4f25-99e4-b3486f415063",
   "metadata": {},
   "source": [
    "## N01 - Point Cloud Representation of Jets\n",
    "\n",
    "In this notebook, we take a look at the LHC Olympics dataset and create a point cloud representation of it in the form of awkward arrays. A visualization of the main kinematic distributions and the particle clouds are also created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2c53b-6ca2-4436-a96d-2720d08326cf",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7cb86-667d-4a7e-95b4-5d8dc54b0695",
   "metadata": {},
   "source": [
    "- The [R&D Dataset](https://zenodo.org/record/4536377) for LHC Olympics 2020 Anomaly Detection Challenge\n",
    "- Additional [background sample](https://zenodo.org/records/8370758)\n",
    "- A total of 100k signal ($W'\\rightarrow XY$) and 1M QCD dijet background events\n",
    "- Signal region defined as $3.3 < m_{jj} < 3.7$ Gev\n",
    "- The signals are generated with $W'$ mass = 3.5 TeV, $X$ mass = 500 GeV and $Y$ mass = 100 GeV\n",
    "- The first 3 * 700 = 2100 columns are the $p_T$, $\\eta$ and $\\phi$ of the constituent particles (up to 700 of them, zero padded)\n",
    "- The last column is the class label (0 = background, 1 = signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fbd95f-f781-46b4-b2a9-0221438c324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward version : 2.4.6\n",
      "vector version  : 1.1.1.post1\n",
      "pyjet version   : 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for nojit\n",
    "import numba as nb\n",
    "\n",
    "# for awkward arrays\n",
    "import awkward as ak\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "# for jet clustering (old method)\n",
    "import pyjet as pj\n",
    "from pyjet import cluster, DTYPE_PTEPM\n",
    "# for jet clustering (new method)\n",
    "#import fastjet as fj\n",
    "\n",
    "proj_dir = \"/global/cfs/projectdirs/m3246/AnomalyDetection/LHCO/\"\n",
    "save_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq\"\n",
    "\n",
    "# print library versions\n",
    "print(f'awkward version : {ak.__version__}')\n",
    "print(f'vector version  : {vector.__version__}')\n",
    "print(f'pyjet version   : {pj.__version__}')\n",
    "\n",
    "output_format = 'parquet'\n",
    "ak_methods = {\n",
    "    'parquet': {\n",
    "        'save': ak.to_parquet,\n",
    "        'load': ak.from_parquet\n",
    "    },\n",
    "    'feather': {\n",
    "        'save': ak.to_feather,\n",
    "        'load': partial(ak.from_feather, memory_map=True)\n",
    "    }\n",
    "}\n",
    "save_fn = ak_methods[output_format]['save']\n",
    "load_fn = ak_methods[output_format]['load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a913b356-6689-4e4d-bcd1-bb75c36ea9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the original shuffled dataset\n",
    "# input_path = os.path.join(proj_dir, \"events_anomalydetection.h5\")\n",
    "# dataset ordered by labels\n",
    "input_path = os.path.join(proj_dir, \"events_anomalydetection_v2.h5\")\n",
    "df = pd.read_hdf(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6179a89-3250-4602-bbcf-d55eb460ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make sure it is the same file from zenodo\n",
    "#!md5sum {input_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45120737-464e-4365-9bd5-8af0fecdcbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 1100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2091</th>\n",
       "      <th>2092</th>\n",
       "      <th>2093</th>\n",
       "      <th>2094</th>\n",
       "      <th>2095</th>\n",
       "      <th>2096</th>\n",
       "      <th>2097</th>\n",
       "      <th>2098</th>\n",
       "      <th>2099</th>\n",
       "      <th>2100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.840249</td>\n",
       "      <td>-2.358095</td>\n",
       "      <td>0.368419</td>\n",
       "      <td>1.317952</td>\n",
       "      <td>-2.464229</td>\n",
       "      <td>0.797502</td>\n",
       "      <td>7.523068</td>\n",
       "      <td>-2.425296</td>\n",
       "      <td>0.405914</td>\n",
       "      <td>6.519727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.953231</td>\n",
       "      <td>-2.312420</td>\n",
       "      <td>-0.239617</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>-2.248381</td>\n",
       "      <td>-0.270049</td>\n",
       "      <td>0.427222</td>\n",
       "      <td>-1.736402</td>\n",
       "      <td>-1.701119</td>\n",
       "      <td>0.347634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.821783</td>\n",
       "      <td>-0.666056</td>\n",
       "      <td>-0.098759</td>\n",
       "      <td>11.534968</td>\n",
       "      <td>-1.580616</td>\n",
       "      <td>2.448051</td>\n",
       "      <td>20.668016</td>\n",
       "      <td>-1.579325</td>\n",
       "      <td>2.474435</td>\n",
       "      <td>13.348019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.489804</td>\n",
       "      <td>-2.238683</td>\n",
       "      <td>2.117016</td>\n",
       "      <td>0.415852</td>\n",
       "      <td>-0.088672</td>\n",
       "      <td>-2.942330</td>\n",
       "      <td>1.489766</td>\n",
       "      <td>-2.257028</td>\n",
       "      <td>1.326838</td>\n",
       "      <td>1.125934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.088889</td>\n",
       "      <td>-1.336245</td>\n",
       "      <td>-0.013672</td>\n",
       "      <td>3.380606</td>\n",
       "      <td>-0.996914</td>\n",
       "      <td>-0.043640</td>\n",
       "      <td>1.360203</td>\n",
       "      <td>-0.825792</td>\n",
       "      <td>0.415315</td>\n",
       "      <td>0.983310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2          3         4         5          6     \\\n",
       "0  1.840249 -2.358095  0.368419   1.317952 -2.464229  0.797502   7.523068   \n",
       "1  0.953231 -2.312420 -0.239617   0.243119 -2.248381 -0.270049   0.427222   \n",
       "2  0.821783 -0.666056 -0.098759  11.534968 -1.580616  2.448051  20.668016   \n",
       "3  0.489804 -2.238683  2.117016   0.415852 -0.088672 -2.942330   1.489766   \n",
       "4  1.088889 -1.336245 -0.013672   3.380606 -0.996914 -0.043640   1.360203   \n",
       "\n",
       "       7         8          9     ...  2091  2092  2093  2094  2095  2096  \\\n",
       "0 -2.425296  0.405914   6.519727  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1 -1.736402 -1.701119   0.347634  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2 -1.579325  2.474435  13.348019  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3 -2.257028  1.326838   1.125934  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4 -0.825792  0.415315   0.983310  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   2097  2098  2099  2100  \n",
       "0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of events: {df.shape[0]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1b2515-03e3-474b-b3cb-4c9e801ec437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of signal events     : 100,000\n",
      "Number of background events : 1,000,000\n"
     ]
    }
   ],
   "source": [
    "# check that there are 1M background and 100k signal events\n",
    "print(f'Number of signal events     : {(df[2100] == 1).sum():,}')\n",
    "print(f'Number of background events : {(df[2100] == 0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e2560-472b-42c1-bbcb-3d02c7b43953",
   "metadata": {},
   "source": [
    "### Jet Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34dd8179-40bb-4a75-9561-55185cce9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aliad.interface.fastjet import JetClusteringTool\n",
    "tool = JetClusteringTool(R=1.0, ptmin=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c48e40-8868-4888-ab83-a637a6b48dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_constituents = {\n",
    "    \"signal\"     : tool.get_constituent_arrays_from_padded_data(df[df[2100] == 1].values),\n",
    "    \"background\" : tool.get_constituent_arrays_from_padded_data(df[df[2100] == 0].values),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6c2931-edf0-4ca8-bea5-905132342057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0.35954976, -0.98441672,  1.40178788, 0.),\n",
       "       (0.35840625, -1.80749536, -0.35914496, 0.),\n",
       "       (0.87615931, -1.90636575,  0.02573433, 0.),\n",
       "       (2.16851616, -1.72313857,  0.01530619, 0.),\n",
       "       (2.2055459 , -1.5062834 ,  0.11419465, 0.)],\n",
       "      dtype=[('pT', '<f8'), ('eta', '<f8'), ('phi', '<f8'), ('mass', '<f8')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first event, first 5 particles\n",
    "jet_constituents[\"signal\"][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e05fed-73fe-4ab7-9ad1-d07dceeb838c",
   "metadata": {},
   "source": [
    "### Extract jet clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef0f2bc-7577-4f1a-a9f1-c0b7d0d9518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_cloud_sig_jets_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_cloud_sig_constituents_jet_1_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_cloud_sig_constituents_jet_2_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_cloud_sig_constituents_jet_3_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_cloud_sig_constituents_jet_4_batch_1.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_1.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_1.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_2.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_2.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_2.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_2.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_2.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_3.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_3.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_3.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_3.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_3.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_4.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_4.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_4.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_4.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_4.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_5.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_5.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_5.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_5.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_5.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_6.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_6.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_6.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_6.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_6.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_7.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_7.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_7.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_7.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_7.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_8.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_8.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_8.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_8.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_8.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_9.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_9.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_9.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_9.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_9.parquet\"\n",
      "Cached existing clustered jet data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_jets_batch_10.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_1_batch_10.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_2_batch_10.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_3_batch_10.parquet\"\n",
      "Cached existing clustered jet constituents data from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq/parquet/row_based/batches/point_bloud_bkg_constituents_jet_4_batch_10.parquet\"\n"
     ]
    }
   ],
   "source": [
    "# need to process in batches due to high memory demand\n",
    "\n",
    "save_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq\"\n",
    "\n",
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_cloud_sig_{obj}_batch_{i}.\" + output_format),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_bloud_bkg_{obj}_batch_{i}.\" + output_format)\n",
    "}\n",
    "if not os.path.exists(os.path.dirname(save_path['signal'])):\n",
    "    os.makedirs(os.path.dirname(save_path['signal']))\n",
    "\n",
    "batchsize = 100000\n",
    "# maximum number of jets to save the constituent information\n",
    "max_jet = 4\n",
    "cache = True\n",
    "save = True\n",
    "all_jets = {}\n",
    "all_constituents = {}\n",
    "for dtype in [\"signal\", \"background\"]:\n",
    "    all_jets[dtype] = []\n",
    "    all_constituents[dtype] = {}\n",
    "    for j in range(max_jet):\n",
    "        all_constituents[dtype][f'j{j+1}'] = []\n",
    "    nbatch = len(jet_constituents[dtype]) // batchsize\n",
    "    if len(jet_constituents[dtype]) % batchsize != 0:\n",
    "        nbatch += 1    \n",
    "    for i in range(nbatch):\n",
    "        i_start = i * batchsize\n",
    "        i_end  = max((i+1)*batchsize, nbatch)\n",
    "        path_jets = save_path[dtype].format(i=i+1, obj=\"jets\")\n",
    "        paths_constituents = []\n",
    "        for j in range(max_jet):\n",
    "            path_constituents = save_path[dtype].format(i=i+1, obj=f\"constituents_jet_{j+1}\")\n",
    "            paths_constituents.append(path_constituents)\n",
    "        # only do jet clustering if not cached\n",
    "        if os.path.exists(path_jets) and all([os.path.exists(path) for path in paths_constituents]) and cache:\n",
    "            jets_batch = None\n",
    "        else:\n",
    "            print(f'Creating clustered jet data for \"{dtype}\" (batch {i+1} / {nbatch})')\n",
    "            jets_batch = tool.get_inclusive_jets_array(jet_constituents[dtype][i_start: i_end])\n",
    "        # convert to awkward array\n",
    "        if os.path.exists(path_jets) and cache:\n",
    "            print(f'Cached existing clustered jet data from \"{path_jets}\"')\n",
    "            jet_array = load_fn(path_jets)\n",
    "        else:\n",
    "            print(f'Converting clustered jet data to awkward arrays')\n",
    "            jet_array = tool.to_awkward_jets_array(jets_batch)\n",
    "            if save:\n",
    "                print(f'Saving clustered jet data to \"{path_jets}\"')\n",
    "                save_fn(jet_array, path_jets)\n",
    "        all_jets[dtype].append(jet_array)\n",
    "        for j in range(max_jet):\n",
    "            path_constituents = paths_constituents[j]\n",
    "            if os.path.exists(path_constituents) and cache:\n",
    "                print(f'Cached existing clustered jet constituents data from \"{path_constituents}\"')\n",
    "                constituent_array = load_fn(path_constituents)\n",
    "            else:\n",
    "                print(f'Creating clustered jet constituents data for \"{dtype}\" (jet {j + 1})')\n",
    "                constituent_array = tool.to_awkward_constituents_array(jets_batch, jet_index=j)\n",
    "                if save:\n",
    "                    print(f'Saving clustered jet constituents data to \"{path_constituents}\"')\n",
    "                    save_fn(constituent_array, path_constituents)\n",
    "            all_constituents[dtype][f'j{j+1}'].append(constituent_array)\n",
    "    all_jets[dtype] = ak.concatenate(all_jets[dtype])\n",
    "    for j in range(max_jet):\n",
    "        all_constituents[dtype][f'j{j+1}'] = ak.concatenate(all_constituents[dtype][f'j{j+1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5e9ac7-8b87-4ff2-b7e0-4c771c68ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined data\n",
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_cloud_sig_{obj}.\" + output_format),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_cloud_bkg_{obj}.\" + output_format)\n",
    "}\n",
    "for dtype in ['signal', 'background']:\n",
    "    save_fn(all_jets[dtype], save_path[dtype].format(obj='jets'))\n",
    "    for j in range(max_jet):\n",
    "        save_fn(all_constituents[dtype][f'j{j+1}'], save_path[dtype].format(obj=f'constituents_jet_{j+1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84436f10-89db-4d77-9ba6-8caf60ee3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jets from first signal event:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[{pt: 1.91e+03, eta: 0.37, phi: -2.66, m: 105},\n",
       " {pt: 1.58e+03, eta: -0.186, phi: 0.237, m: 462},\n",
       " {pt: 485, eta: 2.23, phi: 1.47, m: 32.1},\n",
       " {pt: 26.6, eta: -0.489, phi: 1.54, m: 11.8}]\n",
       "-------------------------------------------------\n",
       "type: 4 * Momentum4D[\n",
       "    pt: float64,\n",
       "    eta: float64,\n",
       "    phi: float64,\n",
       "    m: float64\n",
       "]</pre>"
      ],
      "text/plain": [
       "<MomentumArray4D [{pt: 1.91e+03, eta: 0.37, ...}, ...] type='4 * Momentum4D...'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Jets from first signal event:\\n')\n",
    "all_jets['signal'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec1614f-98e0-4bd6-a777-f6ec1b44de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constituent from leading jet of first signal event:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[{pt: 1.4, eta: 0.184, phi: -2.69, m: -1.49e-08},\n",
       " {pt: 4.69, eta: 0.312, phi: -2.78, m: 5.96e-08},\n",
       " {pt: 51.1, eta: 0.28, phi: -2.75, m: 0},\n",
       " {pt: 1.7, eta: 0.264, phi: -2.72, m: -2.11e-08},\n",
       " {pt: 79.9, eta: 0.307, phi: -2.76, m: 9.54e-07},\n",
       " {pt: 0.967, eta: 0.293, phi: -2.74, m: 0},\n",
       " {pt: 84.1, eta: 0.308, phi: -2.75, m: 0},\n",
       " {pt: 10.7, eta: 0.316, phi: -2.74, m: 1.19e-07},\n",
       " {pt: 1.08, eta: 0.342, phi: -2.76, m: -1.49e-08},\n",
       " {pt: 36, eta: 0.317, phi: -2.73, m: 4.77e-07},\n",
       " ...,\n",
       " {pt: 8.27, eta: 0.356, phi: -2.67, m: 1.19e-07},\n",
       " {pt: 11.7, eta: 0.397, phi: -2.61, m: 0},\n",
       " {pt: 29.1, eta: 0.4, phi: -2.69, m: -3.37e-07},\n",
       " {pt: 38.1, eta: 0.415, phi: -2.66, m: -4.77e-07},\n",
       " {pt: 13.8, eta: 0.396, phi: -2.64, m: 1.69e-07},\n",
       " {pt: 140, eta: 0.401, phi: -2.64, m: 1.91e-06},\n",
       " {pt: 169, eta: 0.406, phi: -2.65, m: -2.7e-06},\n",
       " {pt: 447, eta: 0.401, phi: -2.65, m: -5.39e-06},\n",
       " {pt: 100, eta: 0.407, phi: -2.65, m: -1.35e-06}]\n",
       "--------------------------------------------------\n",
       "type: 23 * Momentum4D[\n",
       "    pt: float64,\n",
       "    eta: float64,\n",
       "    phi: float64,\n",
       "    m: float64\n",
       "]</pre>"
      ],
      "text/plain": [
       "<MomentumArray4D [{pt: 1.4, eta: 0.184, ...}, ..., {...}] type='23 * Moment...'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('constituent from leading jet of first signal event:\\n')\n",
    "all_constituents['signal']['j1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fce8a3-6a8b-43a8-9363-ca291aceea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jets from first background event:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[{pt: 1.59e+03, eta: 0.316, phi: 2.75, m: 38.9},\n",
       " {pt: 1.56e+03, eta: -0.286, phi: -0.448, m: 238},\n",
       " {pt: 90.1, eta: -2.37, phi: 0.45, m: 16.4},\n",
       " {pt: 20.9, eta: -0.965, phi: 0.605, m: 3.79}]\n",
       "--------------------------------------------------\n",
       "type: 4 * Momentum4D[\n",
       "    pt: float64,\n",
       "    eta: float64,\n",
       "    phi: float64,\n",
       "    m: float64\n",
       "]</pre>"
      ],
      "text/plain": [
       "<MomentumArray4D [{pt: 1.59e+03, eta: 0.316, ...}, ...] type='4 * Momentum4...'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Jets from first background event:\\n')\n",
    "all_jets['background'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16120f2-ad05-4828-a385-be0add09e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constituent from leading jet of first background event:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>[{pt: 0.576, eta: 0.488, phi: -2.64, m: 1.05e-08},\n",
       " {pt: 0.271, eta: 0.946, phi: 2.12, m: 0},\n",
       " {pt: 1.44, eta: 0.263, phi: 2.51, m: 0},\n",
       " {pt: 1.33, eta: 0.199, phi: 2.95, m: 2.11e-08},\n",
       " {pt: 8.19, eta: 0.394, phi: 2.71, m: 0},\n",
       " {pt: 3.42, eta: 0.324, phi: 2.73, m: 4.21e-08},\n",
       " {pt: 28.7, eta: 0.336, phi: 2.75, m: 0},\n",
       " {pt: 46.7, eta: 0.322, phi: 2.76, m: 6.74e-07},\n",
       " {pt: 87.6, eta: 0.318, phi: 2.76, m: -1.35e-06},\n",
       " {pt: 120, eta: 0.317, phi: 2.76, m: 1.35e-06},\n",
       " {pt: 34.3, eta: 0.314, phi: 2.76, m: 0},\n",
       " {pt: 122, eta: 0.316, phi: 2.75, m: -1.91e-06},\n",
       " {pt: 296, eta: 0.313, phi: 2.74, m: -3.81e-06},\n",
       " {pt: 838, eta: 0.315, phi: 2.74, m: 1.08e-05}]\n",
       "--------------------------------------------------\n",
       "type: 14 * Momentum4D[\n",
       "    pt: float64,\n",
       "    eta: float64,\n",
       "    phi: float64,\n",
       "    m: float64\n",
       "]</pre>"
      ],
      "text/plain": [
       "<MomentumArray4D [{pt: 0.576, eta: 0.488, ...}, ...] type='14 * Momentum4D[...'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('constituent from leading jet of first background event:\\n')\n",
    "all_constituents['background']['j1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5168759-ea1e-4093-b82d-40c2cb88ccdb",
   "metadata": {},
   "source": [
    "### Prepare features for training\n",
    "\n",
    "\n",
    "Particle-level features\n",
    "\n",
    "- $p_T$, $p_T^\\text{rel}$\n",
    "- $\\eta$, $\\Delta \\eta$\n",
    "- $\\phi$, $\\Delta \\phi$\n",
    "- $\\Delta R$\n",
    "\n",
    "Jet-level features\n",
    "\n",
    "- $p_T$\n",
    "- $\\eta$\n",
    "- $\\phi$\n",
    "- $m$\n",
    "- $N_{\\text{particle}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3882f0-7dbb-4eac-8e7b-a7e1aa755da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for dtype in [\"signal\", \"background\"]:\n",
    "    features[dtype] = {}\n",
    "    for key in all_constituents[dtype]:\n",
    "        features[dtype][key] = {}\n",
    "        # jet features\n",
    "        jet_p4 = ak.sum(all_constituents[dtype][key], axis=-1)\n",
    "        # convert to TeV\n",
    "        features[dtype][key][\"jet_pt\"]   = jet_p4.pt / 1000.\n",
    "        features[dtype][key][\"jet_eta\"]  = jet_p4.eta\n",
    "        features[dtype][key][\"jet_phi\"]  = jet_p4.phi\n",
    "        features[dtype][key][\"jet_m\"]    = jet_p4.m / 1000.\n",
    "        features[dtype][key][\"N\"]        = ak.count(all_constituents[dtype][key], axis=-1)\n",
    "        # particle features\n",
    "        features[dtype][key]['part_pt']        = all_constituents[dtype][key].pt / 1000.\n",
    "        features[dtype][key]['part_eta']       = all_constituents[dtype][key].eta\n",
    "        features[dtype][key]['part_phi']       = all_constituents[dtype][key].phi\n",
    "        features[dtype][key]['part_e']         = all_constituents[dtype][key].e / 1000.\n",
    "        features[dtype][key]['part_rel_pt']    = all_constituents[dtype][key].pt / jet_p4.pt\n",
    "        features[dtype][key]['part_delta_eta'] = all_constituents[dtype][key].deltaeta(jet_p4)\n",
    "        features[dtype][key]['part_delta_phi'] = all_constituents[dtype][key].deltaphi(jet_p4)\n",
    "        features[dtype][key]['part_delta_R']   = all_constituents[dtype][key].deltaR(jet_p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdb76e-917a-4e42-a693-4a66c3b1502a",
   "metadata": {},
   "source": [
    "Fill N-Subjetiness variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fbe8ca-dba8-4d81-b961-873f4fd49661",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(proj_dir, \"events_anomalydetection_v2.features.h5\")\n",
    "df_features = pd.read_hdf(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abbb777a-7f79-4966-8f48-e0fb4566c6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pxj1</th>\n",
       "      <th>pyj1</th>\n",
       "      <th>pzj1</th>\n",
       "      <th>mj1</th>\n",
       "      <th>tau1j1</th>\n",
       "      <th>tau2j1</th>\n",
       "      <th>tau3j1</th>\n",
       "      <th>pxj2</th>\n",
       "      <th>pyj2</th>\n",
       "      <th>pzj2</th>\n",
       "      <th>mj2</th>\n",
       "      <th>tau1j2</th>\n",
       "      <th>tau2j2</th>\n",
       "      <th>tau3j2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1467.239990</td>\n",
       "      <td>611.502014</td>\n",
       "      <td>511.101990</td>\n",
       "      <td>38.896000</td>\n",
       "      <td>8.290660</td>\n",
       "      <td>4.836080</td>\n",
       "      <td>4.260190</td>\n",
       "      <td>1403.579956</td>\n",
       "      <td>-674.551025</td>\n",
       "      <td>-451.670990</td>\n",
       "      <td>237.893997</td>\n",
       "      <td>79.815102</td>\n",
       "      <td>21.010300</td>\n",
       "      <td>16.757601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1211.239990</td>\n",
       "      <td>347.315002</td>\n",
       "      <td>547.963013</td>\n",
       "      <td>389.532013</td>\n",
       "      <td>191.804001</td>\n",
       "      <td>99.562798</td>\n",
       "      <td>70.872200</td>\n",
       "      <td>619.341003</td>\n",
       "      <td>-62.177299</td>\n",
       "      <td>-1944.040039</td>\n",
       "      <td>22.999201</td>\n",
       "      <td>8.042190</td>\n",
       "      <td>6.335090</td>\n",
       "      <td>5.525360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1229.619995</td>\n",
       "      <td>649.857971</td>\n",
       "      <td>8.089170</td>\n",
       "      <td>72.155502</td>\n",
       "      <td>47.168098</td>\n",
       "      <td>37.243198</td>\n",
       "      <td>33.658199</td>\n",
       "      <td>1196.250000</td>\n",
       "      <td>-647.896973</td>\n",
       "      <td>-1283.109985</td>\n",
       "      <td>78.230698</td>\n",
       "      <td>15.292900</td>\n",
       "      <td>13.944200</td>\n",
       "      <td>10.013500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-693.304016</td>\n",
       "      <td>-1046.729980</td>\n",
       "      <td>1716.910034</td>\n",
       "      <td>55.797798</td>\n",
       "      <td>24.788601</td>\n",
       "      <td>6.890150</td>\n",
       "      <td>5.813390</td>\n",
       "      <td>747.961975</td>\n",
       "      <td>994.250000</td>\n",
       "      <td>-412.966003</td>\n",
       "      <td>359.113007</td>\n",
       "      <td>175.209000</td>\n",
       "      <td>103.500999</td>\n",
       "      <td>84.447098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1488.199951</td>\n",
       "      <td>-25.370100</td>\n",
       "      <td>-30.989700</td>\n",
       "      <td>84.891502</td>\n",
       "      <td>26.878799</td>\n",
       "      <td>15.517200</td>\n",
       "      <td>13.260400</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>20.905100</td>\n",
       "      <td>223.630997</td>\n",
       "      <td>77.506500</td>\n",
       "      <td>57.986000</td>\n",
       "      <td>34.147400</td>\n",
       "      <td>26.660601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pxj1         pyj1         pzj1         mj1      tau1j1     tau2j1  \\\n",
       "0 -1467.239990   611.502014   511.101990   38.896000    8.290660   4.836080   \n",
       "1 -1211.239990   347.315002   547.963013  389.532013  191.804001  99.562798   \n",
       "2 -1229.619995   649.857971     8.089170   72.155502   47.168098  37.243198   \n",
       "3  -693.304016 -1046.729980  1716.910034   55.797798   24.788601   6.890150   \n",
       "4 -1488.199951   -25.370100   -30.989700   84.891502   26.878799  15.517200   \n",
       "\n",
       "      tau3j1         pxj2        pyj2         pzj2         mj2      tau1j2  \\\n",
       "0   4.260190  1403.579956 -674.551025  -451.670990  237.893997   79.815102   \n",
       "1  70.872200   619.341003  -62.177299 -1944.040039   22.999201    8.042190   \n",
       "2  33.658199  1196.250000 -647.896973 -1283.109985   78.230698   15.292900   \n",
       "3   5.813390   747.961975  994.250000  -412.966003  359.113007  175.209000   \n",
       "4  13.260400  1415.640015   20.905100   223.630997   77.506500   57.986000   \n",
       "\n",
       "       tau2j2     tau3j2  label  \n",
       "0   21.010300  16.757601    0.0  \n",
       "1    6.335090   5.525360    0.0  \n",
       "2   13.944200  10.013500    0.0  \n",
       "3  103.500999  84.447098    0.0  \n",
       "4   34.147400  26.660601    0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad5823fd-5191-48d8-b7ae-a7da1ec5be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, label in [(\"signal\", 1), (\"background\", 0)]:\n",
    "    df_dtype = df_features[df_features['label'] == label]\n",
    "    for jtype in ['j1', 'j2']:\n",
    "        for tau in ['tau1', 'tau2', 'tau3']:\n",
    "            features[dtype][jtype][f'{tau}'] = df_dtype[f'{tau}{jtype}'].values\n",
    "        ## can use any of tau1, tau2 or tau3 to check for zeros\n",
    "        mask = ((features[dtype][jtype][f'tau1'] > 0) & (features[dtype][jtype][f'tau2'] > 0))\n",
    "        features[dtype][jtype][f'tau12'] = np.where(mask, np.divide(features[dtype][jtype][f'tau2'],\n",
    "                                                                    features[dtype][jtype][f'tau1'], where=mask), 0)\n",
    "        features[dtype][jtype][f'tau23'] = np.where(mask, np.divide(features[dtype][jtype][f'tau3'],\n",
    "                                                                    features[dtype][jtype][f'tau2'], where=mask), 0)\n",
    "        ## remove events with undefined nsubjetiness ratio\n",
    "        #for feature in features[dtype][jtype]:\n",
    "        #    features[dtype][jtype][feature] = features[dtype][jtype][feature][mask]\n",
    "        #features[dtype][jtype][f'tau12'] = features[dtype][jtype][f'tau2'] / features[dtype][jtype][f'tau1']\n",
    "        #features[dtype][jtype][f'tau23'] = features[dtype][jtype][f'tau3'] / features[dtype][jtype][f'tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4565bb-61a0-430e-bfff-a5561e2851dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = {}\n",
    "for dtype in ['signal', 'background']:\n",
    "    sorted_features[dtype] = {'j1':{}, 'j2':{}}\n",
    "    mass_array = [features[dtype]['j1']['jet_m'],\n",
    "                  features[dtype]['j2']['jet_m']]\n",
    "    mass_array = ak.to_numpy(ak.Array(mass_array)).transpose()\n",
    "    sort_index = np.argsort(-mass_array, axis=1)\n",
    "    for feature in ['jet_pt', 'jet_eta', 'jet_phi', 'jet_m', 'N', 'tau1', 'tau2', 'tau3', 'tau12', 'tau23']:\n",
    "        feature_array = [features[dtype]['j1'][feature],\n",
    "                         features[dtype]['j2'][feature]]\n",
    "        feature_array = ak.to_numpy(ak.Array(feature_array)).transpose()\n",
    "        feature_array = np.take_along_axis(feature_array, sort_index, axis=1)\n",
    "        sorted_features[dtype]['j1'][feature] = np.ascontiguousarray(feature_array[:, 0])\n",
    "        sorted_features[dtype]['j2'][feature] = np.ascontiguousarray(feature_array[:, 1])\n",
    "    for feature in ['part_pt', 'part_eta', 'part_phi', 'part_e', 'part_rel_pt',\n",
    "                    'part_delta_eta', 'part_delta_phi', 'part_delta_R']:\n",
    "        sorted_features[dtype]['j1'][feature] = []\n",
    "        sorted_features[dtype]['j2'][feature] = []\n",
    "        feature_array = [features[dtype]['j1'][feature],\n",
    "                         features[dtype]['j2'][feature]]\n",
    "        for i, array in enumerate(zip(*feature_array)):\n",
    "            sorted_features[dtype]['j1'][feature].append(array[sort_index[i][0]])\n",
    "            sorted_features[dtype]['j2'][feature].append(array[sort_index[i][1]])\n",
    "        sorted_features[dtype]['j1'][feature] = sorted_features[dtype]['j1'][feature]\n",
    "        sorted_features[dtype]['j2'][feature] = sorted_features[dtype]['j2'][feature]\n",
    "    sorted_features[dtype] = ak.Array(sorted_features[dtype])\n",
    "    # evaluate mjj\n",
    "    j1_p4 = ak.zip({\n",
    "    \"pt\"  : sorted_features[dtype][\"j1\"][\"jet_pt\"],\n",
    "    \"eta\" : sorted_features[dtype][\"j1\"][\"jet_eta\"],\n",
    "    \"phi\" : sorted_features[dtype][\"j1\"][\"jet_phi\"],\n",
    "    \"m\"   : sorted_features[dtype][\"j1\"][\"jet_m\"]\n",
    "    }, with_name=\"Momentum4D\")\n",
    "    j2_p4 = ak.zip({\n",
    "        \"pt\"  : sorted_features[dtype][\"j2\"][\"jet_pt\"],\n",
    "        \"eta\" : sorted_features[dtype][\"j2\"][\"jet_eta\"],\n",
    "        \"phi\" : sorted_features[dtype][\"j2\"][\"jet_phi\"],\n",
    "        \"m\"   : sorted_features[dtype][\"j2\"][\"jet_m\"]\n",
    "    }, with_name=\"Momentum4D\")\n",
    "    jj_p4 = j1_p4.add(j2_p4)\n",
    "    mjj = np.array(jj_p4.m)\n",
    "    sorted_features[dtype]['jj'] = ak.Array({'mjj': mjj})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498a11a-14ed-4e9c-96de-b0e79c248f38",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efcead-135c-483f-aede-959fe9cbeacb",
   "metadata": {},
   "source": [
    "Inclusive Region (without preselection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7637-ae80-443c-bae1-a68f9779901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"col_based\", f\"point_cloud_W_qq_500_100_official.{output_format}\"),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"col_based\", f\"point_cloud_QCD_qq.{output_format}\")\n",
    "}\n",
    "for dtype in ['signal', 'background']:\n",
    "    dirname = os.path.dirname(save_path[dtype])\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    save_fn(sorted_features[dtype], save_path[dtype])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cae653-49de-41a1-9c58-d32e88fa3721",
   "metadata": {},
   "source": [
    "Signal Region ( $3.3 < m_{jj} < 3.7$ GeV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c177f8-a51d-4cc4-b2c3-053d5bbfd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_W_qq_500_100_official.{output_format}\"),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_QCD_qq.{output_format}\")\n",
    "}\n",
    "SR_sorted_features = {}\n",
    "for dtype in ['signal', 'background']:\n",
    "    mjj = sorted_features[dtype]['jj']['mjj']\n",
    "    mask = np.where((mjj > 3.3) & (mjj < 3.7))\n",
    "    SR_sorted_features[dtype] = sorted_features[dtype][mask]\n",
    "    SR_sorted_features[dtype] = ak.Array(SR_sorted_features[dtype])\n",
    "    save_fn(SR_sorted_features[dtype], save_path[dtype])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447526b-9a1a-4a52-bd2f-e21ee94280e1",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c321cdc-7e43-42c8-b047-fd22ec9464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back saved features\n",
    "save_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq\"\n",
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_W_qq_500_100_official.{output_format}\"),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_QCD_qq.{output_format}\")\n",
    "}\n",
    "features = {}\n",
    "for dtype in save_path:\n",
    "    features[dtype] = load_fn(save_path[dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb8134-e644-4bd1-b5c8-731377d7823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for dtype in [\"signal\", \"background\"]:\n",
    "    data = {\n",
    "        \"lead_jet_pt\"    : np.array(features[dtype][\"j1\"][\"jet_pt\"]),\n",
    "        \"lead_jet_eta\"   : np.array(features[dtype][\"j1\"][\"jet_eta\"]),\n",
    "        \"lead_jet_phi\"   : np.array(features[dtype][\"j1\"][\"jet_phi\"]),\n",
    "        \"lead_jet_m\"     : np.array(features[dtype][\"j1\"][\"jet_m\"]),\n",
    "        \"lead_jet_N\"     : np.array(features[dtype][\"j1\"][\"N\"]),\n",
    "        \"lead_jet_tau1\"  : np.array(features[dtype][\"j1\"][\"tau1\"]),\n",
    "        \"lead_jet_tau2\"  : np.array(features[dtype][\"j1\"][\"tau2\"]),\n",
    "        \"lead_jet_tau3\"  : np.array(features[dtype][\"j1\"][\"tau3\"]),\n",
    "        \"lead_jet_tau12\" : np.array(features[dtype][\"j1\"][\"tau12\"]),\n",
    "        \"lead_jet_tau23\" : np.array(features[dtype][\"j1\"][\"tau23\"]),\n",
    "        \"sub_jet_pt\"     : np.array(features[dtype][\"j2\"][\"jet_pt\"]),\n",
    "        \"sub_jet_eta\"    : np.array(features[dtype][\"j2\"][\"jet_eta\"]),\n",
    "        \"sub_jet_phi\"    : np.array(features[dtype][\"j2\"][\"jet_phi\"]),\n",
    "        \"sub_jet_m\"      : np.array(features[dtype][\"j2\"][\"jet_m\"]),\n",
    "        \"sub_jet_N\"      : np.array(features[dtype][\"j2\"][\"N\"]),\n",
    "        \"sub_jet_tau1\"   : np.array(features[dtype][\"j2\"][\"tau1\"]),\n",
    "        \"sub_jet_tau2\"   : np.array(features[dtype][\"j2\"][\"tau2\"]),\n",
    "        \"sub_jet_tau3\"   : np.array(features[dtype][\"j2\"][\"tau3\"]),\n",
    "        \"sub_jet_tau12\"  : np.array(features[dtype][\"j2\"][\"tau12\"]),\n",
    "        \"sub_jet_tau23\"  : np.array(features[dtype][\"j2\"][\"tau23\"])\n",
    "    }\n",
    "    data['mjj'] =np.array(features[dtype][\"jj\"][\"mjj\"])\n",
    "    dfs[dtype] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54269ad8-e658-4b67-b29f-c934ad9303ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from quickstats.plots import VariableDistributionPlot\n",
    "import matplotlib.pyplot as plt\n",
    "options = {\n",
    "    'mjj': {\n",
    "        'xlabel': '$m_{jj} [GeV]$',\n",
    "        'bin_range': (3200, 3800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_pt': {\n",
    "        'xlabel': '$p_T^{j_1} [GeV]$',\n",
    "        'bin_range': (1200, 2600),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'sub_jet_pt': {\n",
    "        'xlabel': '$p_T^{j_2} [GeV]$',\n",
    "        'bin_range': (1200, 2600),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_m': {\n",
    "        'xlabel': '$m^{j_1} [GeV]$',\n",
    "        'bin_range': (0, 800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'sub_jet_m': {\n",
    "        'xlabel': '$m^{j_2} [GeV]$',\n",
    "        'bin_range': (0, 800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_eta': {\n",
    "        'xlabel': r'$\\eta_{j_1}$'\n",
    "    },\n",
    "    'sub_jet_eta': {\n",
    "        'xlabel': r'$\\eta_{j_2}$'\n",
    "    },\n",
    "    'lead_jet_phi': {\n",
    "        'xlabel': r'$\\phi_{j_1}$'\n",
    "    },\n",
    "    'sub_jet_phi': {\n",
    "        'xlabel': r'$\\phi_{j_2}$'\n",
    "    },\n",
    "    'lead_jet_N': {\n",
    "        'xlabel': '$N_{particle}^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_N': {\n",
    "        'xlabel': '$N_{particle}^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau1': {\n",
    "        'xlabel': r'$\\tau_1^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau1': {\n",
    "        'xlabel': r'$\\tau_1^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau2': {\n",
    "        'xlabel': r'$\\tau_2^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau2': {\n",
    "        'xlabel': r'$\\tau_2^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau3': {\n",
    "        'xlabel': r'$\\tau_3^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau3': {\n",
    "        'xlabel': r'$\\tau_3^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau12': {\n",
    "        'xlabel': r'$\\tau_2^{j_1}/\\tau_1^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau12': {\n",
    "        'xlabel': r'$\\tau_2^{j_2}/\\tau_1^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau23': {\n",
    "        'xlabel': r'$\\tau_3^{j_1}/\\tau_2^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau23': {\n",
    "        'xlabel': r'$\\tau_3^{j_2}/\\tau_2^{j_2}$'\n",
    "    }\n",
    "}\n",
    "label_map = {\n",
    "    'signal'    : 'Singal',\n",
    "    'background': 'Background'\n",
    "}\n",
    "plotter = VariableDistributionPlot(dfs, label_map=label_map)\n",
    "for column in options:\n",
    "    plotter.draw(column,\n",
    "                 **options[column],\n",
    "                 bins=50,\n",
    "                 normalize=True,\n",
    "                 ypad=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa3307-5089-49a6-9927-ee622525bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from quickstats.plots import get_cmap\n",
    "cmap_jet = get_cmap('jet')\n",
    "cmap_jet.set_under('white')\n",
    "\n",
    "def make_jet_image(features, title:str=\"jet\", dimension:int=64,\n",
    "                   xlabel:str=\"$\\Delta \\eta$\", ylabel:str=\"$\\Delta \\phi$\",\n",
    "                   range=((-0.5, 0.5), (-0.5, 0.5)),\n",
    "                   lognorm_range=(1e-4, 1),\n",
    "                   cmap=cmap_jet):\n",
    "    delta_eta = np.array(ak.flatten(features['part_delta_eta'], axis=None))\n",
    "    delta_phi = np.array(ak.flatten(features['part_delta_phi'], axis=None))\n",
    "    rel_pt = np.array(ak.flatten(features['part_rel_pt'], axis=None))\n",
    "    hist2d, x_, y_ = np.histogram2d(delta_eta, \n",
    "                                    delta_phi,\n",
    "                                    bins = (dimension, dimension),\n",
    "                                    range=range,\n",
    "                                    weights=rel_pt)\n",
    "    # normalize the distribution\n",
    "    hist2d_norm = hist2d / np.max(hist2d)\n",
    "    vmin, vmax = lognorm_range\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    extend = np.array(range).flatten()\n",
    "    handle = ax.imshow(hist2d_norm, cmap=cmap,\n",
    "                       norm=LogNorm(vmin=vmin, vmax=vmax), extent=extend)\n",
    "    fig.colorbar(handle, ax=ax)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1ccc7-3c25-43c3-a6f5-87d384680761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dtype in [\"signal\", \"background\"]:\n",
    "    for jtype, label in [(\"j1\", \"Leading Jet\"), (\"j2\", \"Subleading Jet\")]:\n",
    "        make_jet_image(sorted_features[dtype][jtype], title = f\"{dtype.title()}: {label}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac232c6-7435-43d3-9eac-f36527a980b9",
   "metadata": {},
   "source": [
    "## Additional Background Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83639927-3f3f-417f-86fa-b6f1305d749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(proj_dir, \"events_anomalydetection_qcd_extra_inneronly_4vecs_and_features.h5\")\n",
    "df = pd.read_hdf(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872ddca-a3fc-4868-9f83-ba8145b224b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of events: {df.shape[0]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20194a-c97f-4284-98ec-048393cd5832",
   "metadata": {},
   "source": [
    "### Jet Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a3892-fbf3-4396-9810-e6a735623271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aliad.interface.fastjet import JetClusteringTool\n",
    "tool = JetClusteringTool(R=1.0, ptmin=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcef71e-3a04-45f9-9221-c842000b350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_constituents = {\n",
    "    \"extra_background\": tool.get_constituent_arrays_from_padded_data(df[df.columns[:2100]].values)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0828d89-25a9-4eed-a276-88cebfeeb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process in batches due to high memory demand\n",
    "\n",
    "save_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq\"\n",
    "\n",
    "save_path = {\n",
    "    \"extra_background\" : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_cloud_extra_bkg_{obj}_batch_{i}.\" + output_format)\n",
    "}\n",
    "if not os.path.exists(os.path.dirname(save_path['extra_background'])):\n",
    "    os.makedirs(os.path.dirname(save_path['extra_background']))\n",
    "\n",
    "batchsize = 100000\n",
    "# maximum number of jets to save the constituent information\n",
    "max_jet = 4\n",
    "cache = True\n",
    "save = True\n",
    "all_jets = {}\n",
    "all_constituents = {}\n",
    "for dtype in [\"extra_background\"]:\n",
    "    all_jets[dtype] = []\n",
    "    all_constituents[dtype] = {}\n",
    "    for j in range(max_jet):\n",
    "        all_constituents[dtype][f'j{j+1}'] = []\n",
    "    nbatch = len(jet_constituents[dtype]) // batchsize\n",
    "    if len(jet_constituents[dtype]) % batchsize != 0:\n",
    "        nbatch += 1\n",
    "    for i in range(nbatch):\n",
    "        i_start = i * batchsize\n",
    "        i_end  = max((i+1)*batchsize, nbatch)\n",
    "        path_jets = save_path[dtype].format(i=i+1, obj=\"jets\")\n",
    "        paths_constituents = []\n",
    "        for j in range(max_jet):\n",
    "            path_constituents = save_path[dtype].format(i=i+1, obj=f\"constituents_jet_{j+1}\")\n",
    "            paths_constituents.append(path_constituents)\n",
    "        # only do jet clustering if not cached\n",
    "        if os.path.exists(path_jets) and all([os.path.exists(path) for path in paths_constituents]) and cache:\n",
    "            jets_batch = None\n",
    "        else:\n",
    "            print(f'Creating clustered jet data for \"{dtype}\" (batch {i+1} / {nbatch})')\n",
    "            jets_batch = tool.get_inclusive_jets_array(jet_constituents[dtype][i_start: i_end])\n",
    "        # convert to awkward array\n",
    "        if os.path.exists(path_jets) and cache:\n",
    "            print(f'Cached existing clustered jet data from \"{path_jets}\"')\n",
    "            jet_array = load_fn(path_jets)\n",
    "        else:\n",
    "            print(f'Converting clustered jet data to awkward arrays')\n",
    "            jet_array = tool.to_awkward_jets_array(jets_batch)\n",
    "            if save:\n",
    "                print(f'Saving clustered jet data to \"{path_jets}\"')\n",
    "                save_fn(jet_array, path_jets)\n",
    "        all_jets[dtype].append(jet_array)\n",
    "        for j in range(max_jet):\n",
    "            path_constituents = paths_constituents[j]\n",
    "            if os.path.exists(path_constituents) and cache:\n",
    "                print(f'Cached existing clustered jet constituents data from \"{path_constituents}\"')\n",
    "                constituent_array = load_fn(path_constituents)\n",
    "            else:\n",
    "                print(f'Creating clustered jet constituents data for \"{dtype}\" (jet {j + 1})')\n",
    "                constituent_array = tool.to_awkward_constituents_array(jets_batch, jet_index=j)\n",
    "                if save:\n",
    "                    print(f'Saving clustered jet constituents data to \"{path_constituents}\"')\n",
    "                    save_fn(constituent_array, path_constituents)\n",
    "            all_constituents[dtype][f'j{j+1}'].append(constituent_array)\n",
    "    all_jets[dtype] = ak.concatenate(all_jets[dtype])\n",
    "    for j in range(max_jet):\n",
    "        all_constituents[dtype][f'j{j+1}'] = ak.concatenate(all_constituents[dtype][f'j{j+1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409da204-7ffd-4f09-a9ae-7ae93fdcd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined data\n",
    "save_path = {\n",
    "    \"extra_background\" : os.path.join(save_dir, output_format, \"row_based\", \"batches\", \"point_cloud_extra_bkg_{obj}.\" + output_format)\n",
    "}\n",
    "for dtype in ['extra_background']:\n",
    "    save_fn(all_jets[dtype], save_path[dtype].format(obj='jets'))\n",
    "    for j in range(max_jet):\n",
    "        save_fn(all_constituents[dtype][f'j{j+1}'], save_path[dtype].format(obj=f'constituents_jet_{j+1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29426c-65f2-4655-aa8b-4ee42bdd2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for dtype in [\"extra_background\"]:\n",
    "    features[dtype] = {}\n",
    "    for key in all_constituents[dtype]:\n",
    "        features[dtype][key] = {}\n",
    "        # jet features\n",
    "        jet_p4 = ak.sum(all_constituents[dtype][key], axis=-1)\n",
    "        # convert to TeV\n",
    "        features[dtype][key][\"jet_pt\"]   = jet_p4.pt / 1000.\n",
    "        features[dtype][key][\"jet_eta\"]  = jet_p4.eta\n",
    "        features[dtype][key][\"jet_phi\"]  = jet_p4.phi\n",
    "        features[dtype][key][\"jet_m\"]    = jet_p4.m / 1000.\n",
    "        features[dtype][key][\"N\"]        = ak.count(all_constituents[dtype][key], axis=-1)\n",
    "        # particle features\n",
    "        features[dtype][key]['part_pt']        = all_constituents[dtype][key].pt / 1000.\n",
    "        features[dtype][key]['part_eta']       = all_constituents[dtype][key].eta\n",
    "        features[dtype][key]['part_phi']       = all_constituents[dtype][key].phi\n",
    "        features[dtype][key]['part_e']         = all_constituents[dtype][key].e / 1000.\n",
    "        features[dtype][key]['part_rel_pt']    = all_constituents[dtype][key].pt / jet_p4.pt\n",
    "        features[dtype][key]['part_delta_eta'] = all_constituents[dtype][key].deltaeta(jet_p4)\n",
    "        features[dtype][key]['part_delta_phi'] = all_constituents[dtype][key].deltaphi(jet_p4)\n",
    "        features[dtype][key]['part_delta_R']   = all_constituents[dtype][key].deltaR(jet_p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d14f1-712a-44ce-ac9f-f403dda80d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in [\"extra_background\"]:\n",
    "    for jtype in ['j1', 'j2']:\n",
    "        for tau in ['tau1', 'tau2', 'tau3']:\n",
    "            features[dtype][jtype][f'{tau}'] = df[f'{tau}{jtype}'].values\n",
    "        ## can use any of tau1, tau2 or tau3 to check for zeros\n",
    "        mask = ((features[dtype][jtype][f'tau1'] > 0) & (features[dtype][jtype][f'tau2'] > 0))\n",
    "        features[dtype][jtype][f'tau12'] = np.where(mask, np.divide(features[dtype][jtype][f'tau2'],\n",
    "                                                                    features[dtype][jtype][f'tau1'], where=mask), 0)\n",
    "        features[dtype][jtype][f'tau23'] = np.where(mask, np.divide(features[dtype][jtype][f'tau3'],\n",
    "                                                                    features[dtype][jtype][f'tau2'], where=mask), 0)\n",
    "        ## remove events with undefined nsubjetiness ratio\n",
    "        #for feature in features[dtype][jtype]:\n",
    "        #    features[dtype][jtype][feature] = features[dtype][jtype][feature][mask]\n",
    "        #features[dtype][jtype][f'tau12'] = features[dtype][jtype][f'tau2'] / features[dtype][jtype][f'tau1']\n",
    "        #features[dtype][jtype][f'tau23'] = features[dtype][jtype][f'tau3'] / features[dtype][jtype][f'tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e92c3-b004-44f5-80d2-3fe3a014726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = {}\n",
    "for dtype in ['extra_background']:\n",
    "    sorted_features[dtype] = {'j1':{}, 'j2':{}}\n",
    "    mass_array = [features[dtype]['j1']['jet_m'],\n",
    "                  features[dtype]['j2']['jet_m']]\n",
    "    mass_array = ak.to_numpy(ak.Array(mass_array)).transpose()\n",
    "    sort_index = np.argsort(-mass_array, axis=1)\n",
    "    for feature in ['jet_pt', 'jet_eta', 'jet_phi', 'jet_m', 'N', 'tau1', 'tau2', 'tau3', 'tau12', 'tau23']:\n",
    "        feature_array = [features[dtype]['j1'][feature],\n",
    "                         features[dtype]['j2'][feature]]\n",
    "        feature_array = ak.to_numpy(ak.Array(feature_array)).transpose()\n",
    "        feature_array = np.take_along_axis(feature_array, sort_index, axis=1)\n",
    "        sorted_features[dtype]['j1'][feature] = np.ascontiguousarray(feature_array[:, 0])\n",
    "        sorted_features[dtype]['j2'][feature] = np.ascontiguousarray(feature_array[:, 1])\n",
    "    for feature in ['part_pt', 'part_eta', 'part_phi', 'part_e', 'part_rel_pt',\n",
    "                    'part_delta_eta', 'part_delta_phi', 'part_delta_R']:\n",
    "        sorted_features[dtype]['j1'][feature] = []\n",
    "        sorted_features[dtype]['j2'][feature] = []\n",
    "        feature_array = [features[dtype]['j1'][feature],\n",
    "                         features[dtype]['j2'][feature]]\n",
    "        for i, array in enumerate(zip(*feature_array)):\n",
    "            sorted_features[dtype]['j1'][feature].append(array[sort_index[i][0]])\n",
    "            sorted_features[dtype]['j2'][feature].append(array[sort_index[i][1]])\n",
    "        sorted_features[dtype]['j1'][feature] = sorted_features[dtype]['j1'][feature]\n",
    "        sorted_features[dtype]['j2'][feature] = sorted_features[dtype]['j2'][feature]\n",
    "    sorted_features[dtype] = ak.Array(sorted_features[dtype])\n",
    "    # evaluate mjj\n",
    "    j1_p4 = ak.zip({\n",
    "    \"pt\"  : sorted_features[dtype][\"j1\"][\"jet_pt\"],\n",
    "    \"eta\" : sorted_features[dtype][\"j1\"][\"jet_eta\"],\n",
    "    \"phi\" : sorted_features[dtype][\"j1\"][\"jet_phi\"],\n",
    "    \"m\"   : sorted_features[dtype][\"j1\"][\"jet_m\"]\n",
    "    }, with_name=\"Momentum4D\")\n",
    "    j2_p4 = ak.zip({\n",
    "        \"pt\"  : sorted_features[dtype][\"j2\"][\"jet_pt\"],\n",
    "        \"eta\" : sorted_features[dtype][\"j2\"][\"jet_eta\"],\n",
    "        \"phi\" : sorted_features[dtype][\"j2\"][\"jet_phi\"],\n",
    "        \"m\"   : sorted_features[dtype][\"j2\"][\"jet_m\"]\n",
    "    }, with_name=\"Momentum4D\")\n",
    "    jj_p4 = j1_p4.add(j2_p4)\n",
    "    mjj = np.array(jj_p4.m)\n",
    "    sorted_features[dtype]['jj'] = ak.Array({\"mjj\": mjj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007e3ee-a951-40dc-87b5-7eb26d999c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = {\n",
    "    \"extra_background\" : os.path.join(save_dir, output_format, \"col_based\", f\"point_cloud_extra_QCD_qq.{output_format}\")\n",
    "}\n",
    "for dtype in ['extra_background']:\n",
    "    save_fn(sorted_features[dtype], save_path[dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd98784-1e55-457e-9dfc-8dd7181cd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = {\n",
    "    \"extra_background\" : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_extra_QCD_qq.{output_format}\")\n",
    "}\n",
    "SR_sorted_features = {}\n",
    "for dtype in ['extra_background']:\n",
    "    mjj = sorted_features[dtype]['jj']['mjj']\n",
    "    mask = np.where((mjj > 3.3) & (mjj < 3.7))\n",
    "    SR_sorted_features[dtype] = sorted_features[dtype][mask]\n",
    "    SR_sorted_features[dtype] = ak.Array(SR_sorted_features[dtype])\n",
    "    save_fn(SR_sorted_features[dtype], save_path[dtype])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d0c93-5a46-4d3d-9272-c84d45f437eb",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9200ea-b75f-463a-8c53-c38b3164727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back saved features\n",
    "save_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/LHCO_qq\"\n",
    "save_path = {\n",
    "    \"signal\"     : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_W_qq_500_100_official.{output_format}\"),\n",
    "    \"background\" : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_QCD_qq.{output_format}\"),\n",
    "    \"extra_background\" : os.path.join(save_dir, output_format, \"col_based\", f\"SR_point_cloud_extra_QCD_qq.{output_format}\")\n",
    "}\n",
    "features = {}\n",
    "for dtype in save_path:\n",
    "    features[dtype] = load_fn(save_path[dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ace33b-8f57-404d-a273-94725687df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for dtype in features:\n",
    "    data = {\n",
    "        \"lead_jet_pt\"    : np.array(features[dtype][\"j1\"][\"jet_pt\"]),\n",
    "        \"lead_jet_eta\"   : np.array(features[dtype][\"j1\"][\"jet_eta\"]),\n",
    "        \"lead_jet_phi\"   : np.array(features[dtype][\"j1\"][\"jet_phi\"]),\n",
    "        \"lead_jet_m\"     : np.array(features[dtype][\"j1\"][\"jet_m\"]),\n",
    "        \"lead_jet_N\"     : np.array(features[dtype][\"j1\"][\"N\"]),\n",
    "        \"lead_jet_tau1\"  : np.array(features[dtype][\"j1\"][\"tau1\"]),\n",
    "        \"lead_jet_tau2\"  : np.array(features[dtype][\"j1\"][\"tau2\"]),\n",
    "        \"lead_jet_tau3\"  : np.array(features[dtype][\"j1\"][\"tau3\"]),\n",
    "        \"lead_jet_tau12\" : np.array(features[dtype][\"j1\"][\"tau12\"]),\n",
    "        \"lead_jet_tau23\" : np.array(features[dtype][\"j1\"][\"tau23\"]),\n",
    "        \"sub_jet_pt\"     : np.array(features[dtype][\"j2\"][\"jet_pt\"]),\n",
    "        \"sub_jet_eta\"    : np.array(features[dtype][\"j2\"][\"jet_eta\"]),\n",
    "        \"sub_jet_phi\"    : np.array(features[dtype][\"j2\"][\"jet_phi\"]),\n",
    "        \"sub_jet_m\"      : np.array(features[dtype][\"j2\"][\"jet_m\"]),\n",
    "        \"sub_jet_N\"      : np.array(features[dtype][\"j2\"][\"N\"]),\n",
    "        \"sub_jet_tau1\"   : np.array(features[dtype][\"j2\"][\"tau1\"]),\n",
    "        \"sub_jet_tau2\"   : np.array(features[dtype][\"j2\"][\"tau2\"]),\n",
    "        \"sub_jet_tau3\"   : np.array(features[dtype][\"j2\"][\"tau3\"]),\n",
    "        \"sub_jet_tau12\"  : np.array(features[dtype][\"j2\"][\"tau12\"]),\n",
    "        \"sub_jet_tau23\"  : np.array(features[dtype][\"j2\"][\"tau23\"])\n",
    "    }\n",
    "    data['mjj'] =np.array(features[dtype][\"jj\"][\"mjj\"])\n",
    "    dfs[dtype] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918a555-6e87-4400-b35a-4018a44f36cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from quickstats.plots import VariableDistributionPlot\n",
    "import matplotlib.pyplot as plt\n",
    "options = {\n",
    "    'mjj': {\n",
    "        'xlabel': '$m_{jj} [GeV]$',\n",
    "        'bin_range': (3200, 3800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_pt': {\n",
    "        'xlabel': '$p_T^{j_1} [GeV]$',\n",
    "        'bin_range': (1200, 2600),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'sub_jet_pt': {\n",
    "        'xlabel': '$p_T^{j_2} [GeV]$',\n",
    "        'bin_range': (1200, 2600),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_m': {\n",
    "        'xlabel': '$m^{j_1} [GeV]$',\n",
    "        'bin_range': (0, 800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'sub_jet_m': {\n",
    "        'xlabel': '$m^{j_2} [GeV]$',\n",
    "        'bin_range': (0, 800),\n",
    "        'variable_scale': 1000\n",
    "    },\n",
    "    'lead_jet_eta': {\n",
    "        'xlabel': r'$\\eta_{j_1}$'\n",
    "    },\n",
    "    'sub_jet_eta': {\n",
    "        'xlabel': r'$\\eta_{j_2}$'\n",
    "    },\n",
    "    'lead_jet_phi': {\n",
    "        'xlabel': r'$\\phi_{j_1}$'\n",
    "    },\n",
    "    'sub_jet_phi': {\n",
    "        'xlabel': r'$\\phi_{j_2}$'\n",
    "    },\n",
    "    'lead_jet_N': {\n",
    "        'xlabel': '$N_{particle}^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_N': {\n",
    "        'xlabel': '$N_{particle}^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau1': {\n",
    "        'xlabel': r'$\\tau_1^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau1': {\n",
    "        'xlabel': r'$\\tau_1^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau2': {\n",
    "        'xlabel': r'$\\tau_2^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau2': {\n",
    "        'xlabel': r'$\\tau_2^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau3': {\n",
    "        'xlabel': r'$\\tau_3^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau3': {\n",
    "        'xlabel': r'$\\tau_3^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau12': {\n",
    "        'xlabel': r'$\\tau_2^{j_1}/\\tau_1^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau12': {\n",
    "        'xlabel': r'$\\tau_2^{j_2}/\\tau_1^{j_2}$'\n",
    "    },\n",
    "    'lead_jet_tau23': {\n",
    "        'xlabel': r'$\\tau_3^{j_1}/\\tau_2^{j_1}$'\n",
    "    },\n",
    "    'sub_jet_tau23': {\n",
    "        'xlabel': r'$\\tau_3^{j_2}/\\tau_2^{j_2}$'\n",
    "    }\n",
    "}\n",
    "label_map = {\n",
    "    'signal'    : 'Singal',\n",
    "    'background': 'Background',\n",
    "    'extra_background': 'Extra Background'\n",
    "}\n",
    "plotter = VariableDistributionPlot(dfs, label_map=label_map)\n",
    "for column in options:\n",
    "    plotter.draw(column,\n",
    "                 **options[column],\n",
    "                 bins=50,\n",
    "                 normalize=True,\n",
    "                 ypad=0.1)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

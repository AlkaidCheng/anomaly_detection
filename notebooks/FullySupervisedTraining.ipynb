{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f839ba-ee3e-4158-bc9b-4ca596eec626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import awkward as ak\n",
    "\n",
    "input_dir = \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/\"\n",
    "input_path = os.path.join(input_dir, \"point_cloud_features_signal_plus_background.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d44b883-d252-4887-befb-94614ee06257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from quickstats import AbstractObject\n",
    "\n",
    "class PointCloudDataset(AbstractObject):\n",
    "    \"\"\"\n",
    "    This loader will load data in memory. Improvements to be made to load lazily.\n",
    "    \"\"\"\n",
    "    DEFAULT_FEATURE_DICT = {\n",
    "        \"point_coords\"   : [\"part_delta_eta\", \"part_delta_phi\"],\n",
    "        \"point_features\" : [\"part_pt\", \"part_delta_eta\", \"part_delta_phi\", \"part_delta_R\"],\n",
    "        \"jet_features\"   : [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_m\", \"N\", \"tau12\", \"tau23\"]\n",
    "    }\n",
    "\n",
    "    DEFAULT_LABEL_MAP = {\n",
    "        \"background\" : 0,\n",
    "        \"signal\"     : 1\n",
    "    }\n",
    "\n",
    "    DEFAULT_LABEL_FRACTION = {\n",
    "        \"background\" : 1,\n",
    "        \"signal\"     : 1\n",
    "    }\n",
    "    \n",
    "    def __init__(self, filename:str,\n",
    "                 sample_sizes:Optional[Dict]=None,\n",
    "                 label_map:Optional[Dict]=None,\n",
    "                 label_fraction:Optional[Dict]=None,\n",
    "                 feature_dict:Optional[Dict]=None,\n",
    "                 num_jets:int=2, pad_size:int=300,\n",
    "                 shuffle:bool=True, seed:int=2023,\n",
    "                 verbosity:str=\"INFO\"):\n",
    "        super().__init__(verbosity=verbosity)\n",
    "        if feature_dict is None:\n",
    "            self.feature_dict = copy.deepcopy(self.DEFAULT_FEATURE_DICT)\n",
    "        else:\n",
    "            self.feature_dict = copy.deepcopy(feature_dict)\n",
    "        if label_map is None:\n",
    "            self.label_map = copy.deepcopy(self.DEFAULT_LABEL_MAP)\n",
    "        else:\n",
    "            self.label_map = copy.deepcopy(label_map)\n",
    "        if label_fraction is None:\n",
    "            self.label_fraction = copy.deepcopy(self.DEFAULT_LABEL_FRACTION)\n",
    "        else:\n",
    "            self.label_fraction = copy.deepcopy(label_fraction)\n",
    "        if sample_sizes is None:\n",
    "            sample_sizes = {label: None for label in self.label_map}\n",
    "        else:\n",
    "            sample_sizes = {label: sample_sizes.get(label, None) for label in self.label_map}\n",
    "        self.num_jets = num_jets\n",
    "        self.pad_size  = pad_size\n",
    "        self.sample_sizes = sample_sizes\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self._features = {}\n",
    "        self._labels   = None\n",
    "        self.load(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._labels.shape[0]\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_ragged(array):\n",
    "        return isinstance(array.type.content, ak.types.ListType)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_padded_array(array, pad_size:int, clip:bool=True, pad_val:float=0,\n",
    "                         sample_size:Optional[int]=None):\n",
    "        if sample_size is None:\n",
    "            return ak.to_numpy(ak.fill_none(ak.pad_none(array, pad_size, clip=clip), pad_val))\n",
    "        return ak.to_numpy(ak.fill_none(ak.pad_none(array[:sample_size], pad_size, clip=clip), pad_val))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_array(array, sample_size:Optional[int]=None):\n",
    "        if sample_size is None:\n",
    "            return ak.to_numpy(array)\n",
    "        return ak.to_numpy(array[:sample_size])\n",
    "                  \n",
    "    @staticmethod\n",
    "    def get_mask_array(array, pad_size:int, clip:bool=True, sample_size:Optional[int]=None):\n",
    "        if sample_size is None:\n",
    "            return ak.to_numpy(ak.pad_none(array, pad_size, clip=clip)).mask\n",
    "        return ak.to_numpy(ak.pad_none(array[:sample_size], pad_size, clip=clip)).mask\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_partial_array(array, fraction:float=1, shuffle:bool=True):\n",
    "        size = array.shape[0]\n",
    "        new_size = round(size * fraction)\n",
    "        if shuffle:\n",
    "            index = np.arange(new_size, dtype=int)\n",
    "            np.random.shuffle(index)\n",
    "            return array[index]\n",
    "        if fraction == 1:\n",
    "            return array\n",
    "        return array[:new_size]\n",
    "\n",
    "    def load(self, filename:str):\n",
    "        self.stdout.info(f'Loading dataset from \"{filename}\"')\n",
    "        ak_arrays = ak.from_parquet(filename)\n",
    "        features = {}\n",
    "        labels = []\n",
    "        masks  = []\n",
    "        np.random.seed(self.seed)\n",
    "        for label, label_val in self.label_map.items():\n",
    "            self.stdout.info(f'Preparing data for the class \"{label}\"')\n",
    "            if label not in ak_arrays.fields:\n",
    "                raise RuntimeError(f'dataset does not contain data with label \"{label}\"')\n",
    "            label_arrays = ak_arrays[label]\n",
    "            label_size = len(label_arrays)\n",
    "            sample_size = self.sample_sizes[label]\n",
    "            if (sample_size is not None):\n",
    "                if sample_size > label_size:\n",
    "                    raise RuntimeError('can not request more events than is available for the '\n",
    "                                       f'class \"{label}\" (requested = {sample_size}, available = {label_size}')\n",
    "                label_size = sample_size\n",
    "            self.stdout.info(f'Size of class data: {label_size}')\n",
    "            label_fraction = self.label_fraction.get(label, 1)\n",
    "            if label_fraction != 1:\n",
    "                label_size = round(label_size * label_fraction)\n",
    "            labels.append(np.full(label_size, label_val))\n",
    "            mask_arrays = None\n",
    "            for feature_type in self.feature_dict:\n",
    "                self.stdout.info(f'Working on feature type \"{feature_type}\"')\n",
    "                if feature_type not in features:\n",
    "                    features[feature_type] = []\n",
    "                columns = self.feature_dict[feature_type]\n",
    "                jet_feature_arrays = []\n",
    "                jet_mask_arrays = []\n",
    "                for i in range(1, self.num_jets + 1):\n",
    "                    self.stdout.info(f'Jet index: {i}')\n",
    "                    jet_key = f'j{i}'\n",
    "                    feature_arrays = []\n",
    "                    for column in columns:\n",
    "                        self.stdout.info(f'Loading data for the feature \"{column}\"')\n",
    "                        arrays = label_arrays[jet_key][column]\n",
    "                        if self.is_ragged(arrays):\n",
    "                            # only get the mask once per jet\n",
    "                            if (mask_arrays is None) and (len(jet_mask_arrays) == i - 1):\n",
    "                                mask_array = self.get_mask_array(arrays, self.pad_size,\n",
    "                                                                 sample_size=sample_size)\n",
    "                                jet_mask_arrays.append(mask_array)\n",
    "                            arrays = self.get_padded_array(arrays, self.pad_size,\n",
    "                                                           sample_size=sample_size)\n",
    "                        else:\n",
    "                            arrays = self.get_array(arrays, sample_size=sample_size)\n",
    "                        feature_arrays.append(arrays)\n",
    "                    # shape = (nevent, nparticles, nfeatuers)\n",
    "                    feature_arrays = self.get_partial_array(np.stack(feature_arrays, -1),\n",
    "                                                            label_fraction,\n",
    "                                                            shuffle=self.shuffle)\n",
    "                    jet_feature_arrays.append(feature_arrays)\n",
    "                # shape = (nevent, njet, nparticles, nfeatuers)\n",
    "                jet_feature_arrays = np.stack(jet_feature_arrays, axis=1)\n",
    "                features[feature_type].append(jet_feature_arrays)\n",
    "                if jet_mask_arrays:\n",
    "                    mask_arrays = np.stack(jet_mask_arrays, axis=1)\n",
    "            if mask_arrays is not None:\n",
    "                masks.append(mask_arrays)\n",
    "        self.stdout.info(f'Combining data from all classes')\n",
    "        sizes = []\n",
    "        for feature_type in features:\n",
    "            features[feature_type] = np.concatenate(features[feature_type])\n",
    "            sizes.append(features[feature_type].shape[0])\n",
    "        labels = np.concatenate(labels)\n",
    "        if len(set(sizes)) != 1:\n",
    "            raise RuntimeError(\"inconsistent sample size in different feature types\")\n",
    "        if sizes[0] != labels.shape[0]:\n",
    "            raise RuntimeError(\"inconsistent sample size between features and labels\")\n",
    "        if masks:\n",
    "            masks  = np.concatenate(masks)\n",
    "            assert masks.shape[0] == sizes[0]\n",
    "        else:\n",
    "            masks  = None\n",
    "        if self.shuffle:\n",
    "            self.stdout.info(f\"Shuffling events (size = {sizes[0]})\")\n",
    "            index = np.arange(sizes[0])\n",
    "            np.random.shuffle(index)\n",
    "            for feature_type in features:\n",
    "                features[feature_type] = features[feature_type][index]\n",
    "            labels = labels[index]\n",
    "            if masks is not None:\n",
    "                masks = masks[index]\n",
    "        self._features = features\n",
    "        self._labels = labels\n",
    "        self._masks  = masks\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self._features\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def masks(self):\n",
    "        return self._masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6e5c4b-ec19-40e9-bf3b-f84ce0b9661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset from \"/pscratch/sd/c/chlcheng/dataset/anomaly_detection/LHC_Olympics_2020/point_cloud_features_signal_plus_background.parquet\"\n",
      "[INFO] Preparing data for the class \"background\"\n",
      "[INFO] Size of class data: 1000\n",
      "[INFO] Working on feature type \"point_coords\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Working on feature type \"point_features\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"part_pt\"\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Loading data for the feature \"part_delta_R\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"part_pt\"\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Loading data for the feature \"part_delta_R\"\n",
      "[INFO] Working on feature type \"jet_features\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"jet_pt\"\n",
      "[INFO] Loading data for the feature \"jet_eta\"\n",
      "[INFO] Loading data for the feature \"jet_phi\"\n",
      "[INFO] Loading data for the feature \"jet_m\"\n",
      "[INFO] Loading data for the feature \"N\"\n",
      "[INFO] Loading data for the feature \"tau12\"\n",
      "[INFO] Loading data for the feature \"tau23\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"jet_pt\"\n",
      "[INFO] Loading data for the feature \"jet_eta\"\n",
      "[INFO] Loading data for the feature \"jet_phi\"\n",
      "[INFO] Loading data for the feature \"jet_m\"\n",
      "[INFO] Loading data for the feature \"N\"\n",
      "[INFO] Loading data for the feature \"tau12\"\n",
      "[INFO] Loading data for the feature \"tau23\"\n",
      "[INFO] Preparing data for the class \"signal\"\n",
      "[INFO] Size of class data: 100\n",
      "[INFO] Working on feature type \"point_coords\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Working on feature type \"point_features\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"part_pt\"\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Loading data for the feature \"part_delta_R\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"part_pt\"\n",
      "[INFO] Loading data for the feature \"part_delta_eta\"\n",
      "[INFO] Loading data for the feature \"part_delta_phi\"\n",
      "[INFO] Loading data for the feature \"part_delta_R\"\n",
      "[INFO] Working on feature type \"jet_features\"\n",
      "[INFO] Jet index: 1\n",
      "[INFO] Loading data for the feature \"jet_pt\"\n",
      "[INFO] Loading data for the feature \"jet_eta\"\n",
      "[INFO] Loading data for the feature \"jet_phi\"\n",
      "[INFO] Loading data for the feature \"jet_m\"\n",
      "[INFO] Loading data for the feature \"N\"\n",
      "[INFO] Loading data for the feature \"tau12\"\n",
      "[INFO] Loading data for the feature \"tau23\"\n",
      "[INFO] Jet index: 2\n",
      "[INFO] Loading data for the feature \"jet_pt\"\n",
      "[INFO] Loading data for the feature \"jet_eta\"\n",
      "[INFO] Loading data for the feature \"jet_phi\"\n",
      "[INFO] Loading data for the feature \"jet_m\"\n",
      "[INFO] Loading data for the feature \"N\"\n",
      "[INFO] Loading data for the feature \"tau12\"\n",
      "[INFO] Loading data for the feature \"tau23\"\n",
      "[INFO] Combining data from all classes\n",
      "[INFO] Shuffling events (size = 1100)\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = {\n",
    "    \"background\": 1_000,\n",
    "    \"signal\"    :  1_00\n",
    "}\n",
    "dataset = PointCloudDataset(input_path, sample_sizes=sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e2589f-70be-4cb6-97ba-dca0301b3ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 11:27:58.771728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d4cfb6b-bfe0-4f5d-9bf9-1121e0e2b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "points       = dataset.x['point_coords']\n",
    "features     = dataset.x['point_features']\n",
    "jet_features = dataset.x['jet_features']\n",
    "masks        = dataset.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1309f4-c9d8-45a6-978a-81a338d02756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2, 300, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (nevent, njet, nparticle, ncoords)\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b3a34e-2e3e-4657-a283-efadc451c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2, 300, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (nevent, njet, nparticle, nfeatures)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f45b7d9-bb13-4714-b08e-aba61a3b7eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (nevent, njet, njetfeatures)\n",
    "jet_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b325115-7033-4517-a093-9c132cf763f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(A, B):\n",
    "    # A and B has shape (nevent, njet, nparticle, ncoords)\n",
    "    # D has shape (nevent, njet, nparticle, nparticle)\n",
    "    # i.e. the 3rd and 4th dimensions represent the distance between the i-th and j-th particles\n",
    "    with tf.name_scope(\"DMatrix\"):\n",
    "        r_A = tf.reduce_sum(A * A, axis=3, keepdims=True)\n",
    "        r_B = tf.reduce_sum(B * B, axis=3, keepdims=True)\n",
    "        m = tf.matmul(A, tf.transpose(B, perm=(0, 1, 3, 2)))\n",
    "        D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 1, 3, 2))\n",
    "        return D\n",
    "\n",
    "def KNN(features, nn_indices, num_points:int=300, num_jets:int=2, K:int=16):\n",
    "    \"\"\"\n",
    "    features: Tensor of shape (nevent, njet, nparticle, nfeature)\n",
    "        Features of the point cloud particles\n",
    "    nn_indices: Tensor of shape (nevent, njet, nparticle, nneighbor)\n",
    "        Indices of the nearest-neighbors of the point cloud particles\n",
    "    num_points: int\n",
    "        Number of partcles in the point cloud\n",
    "    num_jets: int\n",
    "        Number of jets in an event\n",
    "    K : int\n",
    "        Number of nearest-neighbors\n",
    "    nn: tensor of shape (nevent, njet, nparticle, k)\n",
    "        Indices of the nearest-neighbors\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"KNN\"):\n",
    "        feature_shape = tf.shape(features)\n",
    "        batchsize = feature_shape[0]\n",
    "        num_features = feature_shape[-1]\n",
    "        batch_indices = tf.tile(tf.reshape(tf.range(batchsize), (-1, 1, 1, 1, 1)),\n",
    "                                (1, num_jets, num_points, K, 1))\n",
    "        indices = tf.concat([batch_indices, tf.expand_dims(nn_indices, axis=4)], axis=4)\n",
    "        rs_indices = tf.reshape(indices, (batchsize * num_jets, num_points, K, 2))\n",
    "        rs_features      = tf.reshape(features, (batchsize * num_jets, num_points, num_features))\n",
    "        # shape = (nevent, njet, nparticle, nneighbor, nfeature)\n",
    "        return tf.reshape(tf.gather_nd(rs_features, rs_indices), (batchsize, num_jets, num_points, K, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2b84f1-69a1-499b-a314-36ece7d2d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 11:28:11.017494: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "D = get_distance_matrix(points, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cc8efa-7440-466a-92f9-4493a4f9db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 16\n",
    "_, nn_indices = tf.nn.top_k(-D, k = K + 1)\n",
    "# remove the first index which is always the particle itself\n",
    "nn_indices = nn_indices[:, :, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b975b26d-7564-4c85-8d7b-4d08ac07ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_fts = KNN(features, nn_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21e0eb7-4f89-4cdb-b48c-e08053376575",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44365e51-ad40-43fd-81d8-49d4d6b78a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:40:49.800834: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "coord_shift = tf.multiply(999., tf.cast(masks, dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e28e72-357b-48ef-84fe-f76689957453",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_params = [\n",
    "    (16, (64, 64, 64)),\n",
    "    (16, (128, 128, 128)),\n",
    "    (16, (256, 256, 256)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc422dc9-c5fa-4b39-9d84-1cded73aaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quickstats import semistaticmethod, AbstractObject\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class ModifiedParticleNet(AbstractObject):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 verbosity:str=\"INFO\"):\n",
    "        super().__init__(verbosity=verbosity)):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_distance_matrix(A, B):\n",
    "        # A and B has shape (nevent, njet, nparticle, ncoords)\n",
    "        # D has shape (nevent, njet, nparticle, nparticle)\n",
    "        # i.e. the 3rd and 4th dimensions represent the distance between the i-th and j-th particles\n",
    "        with tf.name_scope(\"DMatrix\"):\n",
    "            r_A = tf.reduce_sum(A * A, axis=3, keepdims=True)\n",
    "            r_B = tf.reduce_sum(B * B, axis=3, keepdims=True)\n",
    "            m = tf.matmul(A, tf.transpose(B, perm=(0, 1, 3, 2)))\n",
    "            D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 1, 3, 2))\n",
    "            return D\n",
    "\n",
    "    @staticmethod\n",
    "    def KNN(features, nn_indices, num_points:int=300, num_jets:int=2, K:int=16):\n",
    "        \"\"\"\n",
    "        features: Tensor of shape (nevent, njet, nparticle, nfeature)\n",
    "            Features of the point cloud particles\n",
    "        nn_indices: Tensor of shape (nevent, njet, nparticle, nneighbor)\n",
    "            Indices of the nearest-neighbors of the point cloud particles\n",
    "        num_points: int\n",
    "            Number of partcles in the point cloud\n",
    "        num_jets: int\n",
    "            Number of jets in an event\n",
    "        K : int\n",
    "            Number of nearest-neighbors\n",
    "        nn: tensor of shape (nevent, njet, nparticle, k)\n",
    "            Indices of the nearest-neighbors\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"KNN\"):\n",
    "            feature_shape = tf.shape(features)\n",
    "            batchsize = feature_shape[0]\n",
    "            num_features = feature_shape[-1]\n",
    "            batch_indices = tf.tile(tf.reshape(tf.range(batchsize), (-1, 1, 1, 1, 1)),\n",
    "                                    (1, num_jets, num_points, K, 1))\n",
    "            indices = tf.concat([batch_indices, tf.expand_dims(nn_indices, axis=4)], axis=4)\n",
    "            rs_indices = tf.reshape(indices, (batchsize * num_jets, num_points, K, 2))\n",
    "            rs_features      = tf.reshape(features, (batchsize * num_jets, num_points, num_features))\n",
    "            # shape = (nevent, njet, nparticle, nneighbor, nfeature)\n",
    "            return tf.reshape(tf.gather_nd(rs_features, rs_indices), (batchsize, num_jets, num_points, K, num_features))\n",
    "            \n",
    "    @semistaticmethod\n",
    "    def EdgeConv(self, points, features,  channels,\n",
    "                 num_jets:int=2,\n",
    "                 num_points:int=300,\n",
    "                 K:int=16,\n",
    "                 with_bn:bool=True,\n",
    "                 activation:str='relu',\n",
    "                 pooling:str='average'):\n",
    "        \"\"\"\n",
    "            points: Tensor of shape (nevent, njet, nparticle, ncoords)\n",
    "                Coordinates of the point cloud particles\n",
    "            features: Tensor of shape (nevent, njet, nparticle, nfeatures)\n",
    "                Features of the point cloud particles\n",
    "            num_points: int\n",
    "                Number of partcles in the point cloud\n",
    "            num_jets: int\n",
    "                Number of jets in an event\n",
    "            K: int\n",
    "                Number of nearest-neighbors\n",
    "        \"\"\"\n",
    "        with tf.name_scope('EdgeConv'):\n",
    "            D = self.get_distance_matrix(points, points)\n",
    "            # get indices of nearest neighbors\n",
    "            _, indices = tf.nn.top_k(-D, k = K + 1)\n",
    "            # remove the first index which is always the particle itself\n",
    "            indices = indices[:, :, :, 1:]\n",
    "\n",
    "            knn_fts = self.KNN(features, indices, num_points=num_points,\n",
    "                               num_jets=num_jets, K=K)\n",
    "            >>>>>>>\n",
    "            knn_fts_center = tf.tile(tf.expand_dims(features, axis=2), (1, 1, K, 1))  # (N, P, K, C)\n",
    "            >>>>>>>\n",
    "            knn_fts = tf.concat([knn_fts_center, tf.subtract(knn_fts, knn_fts_center)], axis=-1)  # (N, P, K, 2*C)\n",
    "    \n",
    "            x = knn_fts\n",
    "            for idx, channel in enumerate(channels):\n",
    "                x = keras.layers.Conv2D(channel, kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "                                        use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_conv%d' % (name, idx))(x)\n",
    "                if with_bn:\n",
    "                    x = keras.layers.BatchNormalization(name='%s_bn%d' % (name, idx))(x)\n",
    "                if activation:\n",
    "                    x = keras.layers.Activation(activation, name='%s_act%d' % (name, idx))(x)\n",
    "            >>>>>>>\n",
    "            if pooling == 'max':\n",
    "                fts = tf.reduce_max(x, axis=2)  # (N, P, C')\n",
    "            else:\n",
    "                fts = tf.reduce_mean(x, axis=2)  # (N, P, C')\n",
    "            >>>>>>>\n",
    "            # shortcut\n",
    "            sc = keras.layers.Conv2D(channels[-1], kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "                                     use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_sc_conv' % name)(tf.expand_dims(features, axis=2))\n",
    "            if with_bn:\n",
    "                sc = keras.layers.BatchNormalization(name='%s_sc_bn' % name)(sc)\n",
    "            sc = tf.squeeze(sc, axis=2)\n",
    "    \n",
    "            if activation:\n",
    "                return keras.layers.Activation(activation, name='%s_sc_act' % name)(sc + fts)  # (N, P, C')\n",
    "            else:\n",
    "                return sc + fts\n",
    "\n",
    "    \n",
    "    def get_model(points, features=None, mask=None, setting=None, name='particle_net'):\n",
    "        # points : (N, P, C_coord)\n",
    "        # features:  (N, P, C_features), optional\n",
    "        # mask: (N, P, 1), optinal\n",
    "    \n",
    "        with tf.name_scope(name):\n",
    "            if features is None:\n",
    "                features = points\n",
    "    \n",
    "            if mask is not None:\n",
    "                mask = tf.cast(tf.not_equal(mask, 0), dtype='float32')  # 1 if valid\n",
    "                coord_shift = tf.multiply(999., tf.cast(tf.equal(mask, 0), dtype='float32'))  # make non-valid positions to 99\n",
    "    \n",
    "            fts = tf.squeeze(keras.layers.BatchNormalization(name='%s_fts_bn' % name)(tf.expand_dims(features, axis=2)), axis=2)\n",
    "            for layer_idx, layer_param in enumerate(setting.conv_params):\n",
    "                K, channels = layer_param\n",
    "                pts = tf.add(coord_shift, points) if layer_idx == 0 else tf.add(coord_shift, fts)\n",
    "                fts = edge_conv(pts, fts, setting.num_points, K, channels, with_bn=True, activation='relu',\n",
    "                                pooling=setting.conv_pooling, name='%s_%s%d' % (name, 'EdgeConv', layer_idx))\n",
    "    \n",
    "            if mask is not None:\n",
    "                fts = tf.multiply(fts, mask)\n",
    "    \n",
    "            pool = tf.reduce_mean(fts, axis=1)  # (N, C)\n",
    "    \n",
    "            if setting.fc_params is not None:\n",
    "                x = pool\n",
    "                for layer_idx, layer_param in enumerate(setting.fc_params):\n",
    "                    units, drop_rate = layer_param\n",
    "                    x = keras.layers.Dense(units, activation='relu')(x)\n",
    "                    if drop_rate is not None and drop_rate > 0:\n",
    "                        x = keras.layers.Dropout(drop_rate)(x)\n",
    "                out = keras.layers.Dense(setting.num_class, activation='softmax')(x)\n",
    "                return out  # (N, num_classes)\n",
    "            else:\n",
    "                return pool\n",
    "\n",
    "        >>>\n",
    "            setting = _DotDict()\n",
    "            setting.num_class = num_classes\n",
    "            # conv_params: list of tuple in the format (K, (C1, C2, C3))\n",
    "            setting.conv_params = [\n",
    "                (16, (64, 64, 64)),\n",
    "                (16, (128, 128, 128)),\n",
    "                (16, (256, 256, 256)),\n",
    "                ]\n",
    "            # conv_pooling: 'average' or 'max'\n",
    "            setting.conv_pooling = 'average'\n",
    "            # fc_params: list of tuples in the format (C, drop_rate)\n",
    "            setting.fc_params = [(256, 0.1)]\n",
    "            setting.num_points = input_shapes['points'][0]\n",
    "        \n",
    "            points = keras.Input(name='points', shape=input_shapes['points'])\n",
    "            features = keras.Input(name='features', shape=input_shapes['features']) if 'features' in input_shapes else None\n",
    "            mask = keras.Input(name='mask', shape=input_shapes['mask']) if 'mask' in input_shapes else None\n",
    "            outputs = _particle_net_base(points, features, mask, setting, name='ParticleNet')\n",
    "        \n",
    "            return keras.Model(inputs=[points, features, mask], outputs=outputs, name='ParticleNet')        \n",
    "        >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53236324-cbf7-43e8-abf0-0a3177f32d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
